{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88abfe4-b7b5-4557-964e-6af153f32155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: /home/jovyan/ocp/checkpoints/2021-09-17-18-42-08\n",
      "  commit: f4a0f08\n",
      "  identifier: ''\n",
      "  logs_dir: /home/jovyan/ocp/logs/tensorboard/2021-09-17-18-42-08\n",
      "  print_every: 100\n",
      "  results_dir: /home/jovyan/ocp/results/2021-09-17-18-42-08\n",
      "  seed: null\n",
      "  timestamp_id: 2021-09-17-18-42-08\n",
      "dataset:\n",
      "  grad_target_mean: 0.0\n",
      "  grad_target_std: 2.887317180633545\n",
      "  normalize_labels: true\n",
      "  src: /home/jovyan/ocp/data/OC20/s2ef/all/test_id/\n",
      "  target_mean: -0.7554450631141663\n",
      "  target_std: 2.887317180633545\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: gemnet_t\n",
      "model_attributes:\n",
      "  activation: silu\n",
      "  cbf:\n",
      "    name: spherical_harmonics\n",
      "  cutoff: 6.0\n",
      "  direct_forces: true\n",
      "  emb_size_atom: 512\n",
      "  emb_size_bil_trip: 64\n",
      "  emb_size_cbf: 16\n",
      "  emb_size_edge: 512\n",
      "  emb_size_rbf: 16\n",
      "  emb_size_trip: 64\n",
      "  envelope:\n",
      "    exponent: 5\n",
      "    name: polynomial\n",
      "  extensive: true\n",
      "  max_neighbors: 50\n",
      "  num_after_skip: 2\n",
      "  num_atom: 3\n",
      "  num_before_skip: 1\n",
      "  num_blocks: 3\n",
      "  num_concat: 1\n",
      "  num_radial: 128\n",
      "  num_spherical: 7\n",
      "  otf_graph: false\n",
      "  output_init: HeOrthogonal\n",
      "  rbf:\n",
      "    name: gaussian\n",
      "  regress_forces: true\n",
      "  scale_file: /home/jovyan/ocp/configs/s2ef/all/gemnet/scaling_factors/gemnet-dT.json\n",
      "optim:\n",
      "  batch_size: 32\n",
      "  clip_grad_norm: 10\n",
      "  ema_decay: 0.999\n",
      "  energy_coefficient: 1\n",
      "  eval_batch_size: 32\n",
      "  eval_every: 5000\n",
      "  factor: 0.8\n",
      "  force_coefficient: 100\n",
      "  loss_energy: mae\n",
      "  loss_force: l2mae\n",
      "  lr_initial: 0.0005\n",
      "  max_epochs: 80\n",
      "  mode: min\n",
      "  num_workers: 2\n",
      "  optimizer: AdamW\n",
      "  optimizer_params:\n",
      "    amsgrad: true\n",
      "  patience: 3\n",
      "  scheduler: ReduceLROnPlateau\n",
      "slurm: {}\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "val_dataset:\n",
      "  src: /home/jovyan/ocp/data/OC20/s2ef/all/val_id/\n",
      "\n",
      "2021-09-17 18:43:06 (INFO): Loading dataset: trajectory_lmdb\n",
      "2021-09-17 18:43:07 (INFO): Loading model: gemnet_t\n",
      "2021-09-17 18:43:10 (INFO): Loaded GemNetT with 31671825 parameters.\n",
      "2021-09-17 18:43:10 (INFO): Loading checkpoint from: /home/jovyan/shared-scratch/Brook/gemnet_t_direct_h512_all.pt\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import random\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import functools\n",
    "import yaml\n",
    "\n",
    "from dask_kubernetes import KubeCluster\n",
    "from joblib import Memory\n",
    "from dask.distributed import Client\n",
    "from calculator_upload_empty import predict_E\n",
    "from ocpmodels.models.dimenet_plus_plus import DimeNetPlusPlusWrap\n",
    "\n",
    "import os\n",
    "\n",
    "from enumeration_helper_script import enumerate_surface_wrap\n",
    "from tqdm import tqdm\n",
    "from ocdata.adsorbates import Adsorbate\n",
    "from ocdata.bulk_obj import Bulk\n",
    "from ocdata.base_atoms.pkls import BULK_PKL, ADSORBATE_PKL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ef3089-2c7f-414e-bec4-6d0724bb5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate some helpful accessories\n",
    "#------------------------------------------------------------------------------\n",
    "name_to_num_dict = {1: 'H', 2: 'He', 3: 'Li', 4: 'Be', 5: 'B', 6: 'C', 7: 'N',\n",
    "                    8: 'O', 9: 'F', 10: 'Ne', 11: 'Na', 12: 'Mg', 13: 'Al', 14: 'Si',\n",
    "                    15: 'P', 16: 'S', 17: 'Cl', 18: 'Ar', 19: 'K', 20: 'Ca', 21: 'Sc',\n",
    "                    22: 'Ti', 23: 'V', 24: 'Cr', 25: 'Mn', 26: 'Fe', 27: 'Co', 28: 'Ni',\n",
    "                    29: 'Cu', 30: 'Zn', 31: 'Ga', 32: 'Ge', 33: 'As', 34: 'Se', 35: 'Br', \n",
    "                    36: 'Kr', 37: 'Rb', 38: 'Sr', 39: 'Y', 40: 'Zr', 41: 'Nb', 42: 'Mo',\n",
    "                    43: 'Tc', 44: 'Ru', 45: 'Rh', 46: 'Pd', 47: 'Ag', 48: 'Cd', 49: 'In',\n",
    "                    50: 'Sn', 51: 'Sb', 52: 'Te', 53: 'I', 54: 'Xe', 55: 'Cs', 56: 'Ba', \n",
    "                    57: 'La', 58: 'Ce', 59: 'Pr', 60: 'Nd', 61: 'Pm', 62: 'Sm', 63: 'Eu', \n",
    "                    64: 'Gd', 65: 'Tb', 66: 'Dy', 67: 'Ho', 68: 'Er', 69: 'Tm', 70: 'Yb',\n",
    "                    71: 'Lu', 72: 'Hf', 73: 'Ta', 74: 'W', 75: 'Re', 76: 'Os', 77: 'Ir', \n",
    "                    78: 'Pt', 79: 'Au', 80: 'Hg', 81: 'Tl', 82: 'Pb', 83: 'Bi', 84: 'Po',\n",
    "                    85: 'At', 86: 'Rn', 87: 'Fr', 88: 'Ra', 89: 'Ac', 90: 'Th', 91: 'Pa',\n",
    "                    92: 'U', 93: 'Np', 94: 'Pu', 95: 'Am', 96: 'Cm', 97: 'Bk', 98: 'Cf',\n",
    "                    99: 'Es', 100: 'Fm', 101: 'Md', 102: 'No', 103: 'Lr', 104: 'Rf',\n",
    "                    105: 'Db', 106: 'Sg', 107: 'Bh', 108: 'Hs', 109: 'Mt', 110: 'Ds', \n",
    "                    111: 'Rg', 112: 'Cn', 113: 'Nh', 114: 'Fl', 115: 'Mc', 116: 'Lv',\n",
    "                    117: 'Ts', 118: 'Og'}\n",
    "\n",
    "transition_metal1 =  list(range(21,31))\n",
    "transition_metal2 = list(range(39, 49))\n",
    "transition_metal3 = list(range(72, 80))\n",
    "transition_metals = [*transition_metal1, *transition_metal2, *transition_metal3]\n",
    "# you can set elements_to_include = transition_metals\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INPUTS!!\n",
    "# ----------------------------------------------------------------------------\n",
    "# Bulk inputs\n",
    "direct = False # boolean True = direct method, False = relaxations\n",
    "mpids_to_slabs = True # True = use mpid_list to generate aslabs, False = use element list and num_of_els\n",
    "elements_to_include = [29]# list of desired atomic numbers\n",
    "mpid_list = ['mp-101']#, 'mp-2', 'mp-30', 'mp-74', 'mp-126', 'mp-101', 'mp-23', 'mp-124', 'mp-102']\n",
    "num_of_els = [1] # [1,2] = unary and binary\n",
    "mpids_to_exclude = ['mp-672234', 'mp-632250', 'mp-754514', 'mp-570747', 'mp-12103', 'mp-25', \n",
    "                    'mp-672233', 'mp-568584', 'mp-154', 'mp-999498', 'mp-14', 'mp-96', 'mp-1080711', \n",
    "                    'mp-1008394', 'mp-22848', 'mp-160', 'mp-1198724'] #these are odd and cause failures\n",
    "\n",
    "# Adsorbate inputs\n",
    "adsorbates_smile_list = ['*OH']\n",
    "\n",
    "# File paths\n",
    "worker_spec_path = '/home/jovyan/shared-scratch/Brook/worker-spec-relax.yml'\n",
    "your_shared_scratch_path = '/home/jovyan/shared-scratch/Brook' # this will be used to create a cache dir in your folder\n",
    "\n",
    "# Other\n",
    "num_workers = 50 # should be scaled with workload. Use 1 to troubleshoot. 10 is a good place to start\n",
    "max_size = 1000 # This is the maximum number of atoms you will allow in your slabs. Any greater will return size = Number rather than a prediction.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pre-work / organization\n",
    "# -----------------------------------------------------------------------------\n",
    "# convert el nums to names\n",
    "elnames_to_include = [name_to_num_dict[el] for el in elements_to_include] \n",
    "\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9794c5-cf46-4a5a-ba3d-7a427af62c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "/opt/conda/lib/python3.8/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44111 instead\n",
      "  warnings.warn(\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.42.7.156:36903\n",
      "distributed.scheduler - INFO -   dashboard at:                    :44111\n",
      "distributed.deploy.adaptive - INFO - Adaptive scaling started: minimum=1 maximum=50\n",
      "distributed.scheduler - INFO - Receive client connection: Client-32ddca5a-17e7-11ec-834f-eae5c63ff668\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "# Create the workers\n",
    "# -----------------------------------------------------------------------------\n",
    "# Set the up a kube dask cluster \n",
    "cluster = KubeCluster(worker_spec_path, deploy_mode='local')\n",
    "cluster.adapt(minimum=1, maximum = num_workers, interval = '30000 ms')\n",
    "client = Client(cluster)\n",
    "\n",
    "def _worker_upload(dask_worker, *, data, fname):\n",
    "    dask_worker.loop.add_callback(\n",
    "    callback=dask_worker.upload_file,\n",
    "    comm=None,  # not used\n",
    "    filename=fname,\n",
    "    data=data,\n",
    "    load=True)\n",
    "\n",
    "## Upload code to every worker as they start/restart\n",
    "fname_ = ['enumeration_helper_script.py', 'calculator_upload_empty.py']\n",
    "for fname in fname_:\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = f.read()\n",
    "        client.register_worker_callbacks(\n",
    "            setup=functools.partial(\n",
    "                _worker_upload, data=data, fname=fname,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Set up the cache directory - this will also be the local directory on each worker that will store cached results\n",
    "location_preds = your_shared_scratch_path + '/cachedir_predictions_AIChE1'\n",
    "location_slabs = your_shared_scratch_path + '/cachedir_slabs_AIChEpost'\n",
    "memory_preds = Memory(location_preds,verbose=1)\n",
    "memory_slabs = Memory(location_slabs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec73bd62-5aed-4e82-a740-91682ea2d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pkl of bulks and their associated info\n",
    "with open(BULK_PKL, 'rb') as f:\n",
    "    inv_index = pickle.load(f)\n",
    "bulks = []\n",
    "\n",
    "# Grab the desired bulk atoms object\n",
    "if mpids_to_slabs:\n",
    "    num_of_els = [1,2,3]\n",
    "    for k in num_of_els:\n",
    "        for itm in inv_index[k]:\n",
    "            if itm[1] not in mpids_to_exclude and itm[1] in mpid_list:\n",
    "                 bulks.append(itm)\n",
    "else:\n",
    "    for k in num_of_els:\n",
    "        for itm in inv_index[k]: \n",
    "            els = itm[0].get_chemical_symbols() # Grab the chemical symbol of all\n",
    "            els = np.unique(els)\n",
    "            if itm[1] not in mpids_to_exclude and set(els).issubset(elnames_to_include):\n",
    "                bulks.append(itm)\n",
    "\n",
    "adsorbates_smile_list = ['*C']\n",
    "\n",
    "# Open pkl of adsorbates and grab info for the ones in the smile list         \n",
    "with open(ADSORBATE_PKL, 'rb') as f:\n",
    "    inv_index = pickle.load(f)\n",
    "ads_idx = [key for key in inv_index if inv_index[key][1] in adsorbates_smile_list]\n",
    "adsorbates_obj = [Adsorbate(ADSORBATE_PKL, specified_index = idx) for idx in ads_idx]\n",
    "\n",
    "\n",
    "#create a dask bag of adsorbates\n",
    "adsorbate_bag = db.from_sequence(adsorbates_obj)\n",
    "\n",
    "#create a dask bag of bulks\n",
    "bulk_bag = db.from_sequence(bulks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c43dca-1ca3-400c-affe-e327dabceb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.0.133:40583', name: 0, memory: 0, processing: 2>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.0.133:40583\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.0.134:43509', name: 3, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.0.134:43509\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.0.135:38787', name: 4, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.0.135:38787\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.18.64:35715', name: 5, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.18.64:35715\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.0.136:39991', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.0.136:39991\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.42.0.137:44205', name: 2, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.42.0.137:44205\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "# generate the adslab mappings\n",
    "surfaces_bag = bulk_bag.map(memory_slabs.cache(enumerate_surface_wrap)).flatten()\n",
    "surface_ads_combos = surfaces_bag.product(adsorbate_bag).repartition(npartitions=num_workers*20).compute()\n",
    "\n",
    "#Load surfaces into dask bag\n",
    "surfaces = db.from_sequence(surface_ads_combos)\n",
    "# generate prediction mappings\n",
    "predictions_bag = surfaces.map(memory_preds.cache(predict_E), max_size, direct)\n",
    "# execute operations (go to all work)\n",
    "predictions = predictions_bag.compute() # change to .compute() to push to local RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d2d56-d790-4b64-a915-e92366611d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
